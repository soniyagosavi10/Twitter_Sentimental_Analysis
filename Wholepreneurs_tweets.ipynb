{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install twitter\n",
    "#!pip install emoji\n",
    "#!pip install emot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/larissa/Desktop/Canada/Langara/3rd_term/CPSC_4820/Group_Project\n",
      "Apikey.py\n",
      "Brain_Enhancement_Food_Collection.json\n",
      "Client Workbook - wholepreneurlifestyle.com.xlsx\n",
      "Competitors_Collection.json\n",
      "Health_Collection.json\n",
      "Keywords.docx\n",
      "Meal_Replacement_Collection.json\n",
      "Others_Collection.json\n",
      "PostgreSQL_Python_Connection.ipynb\n",
      "ProjectProposal-CPSC4820.pdf\n",
      "Sample Project Proposal - Group 4.pdf\n",
      "Sentiment_Collection.json\n",
      "Stress_Relief_Collection.json\n",
      "Vegan_Organic_Collection.json\n",
      "\u001b[34mWholepreneur\u001b[m\u001b[m\n",
      "Wholepreneurs_tweets.ipynb\n",
      "Workout_Collection.json\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m\n",
      "sentiment.xlsx\n"
     ]
    }
   ],
   "source": [
    "#os.chdir(\"./Desktop/Canada/Langara/2nd_term/CPSC_4810/\")\n",
    "print(os.getcwd())\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larissa/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.4\n"
     ]
    }
   ],
   "source": [
    "import requests, pprint, json, datetime, time, collections\n",
    "from datetime import datetime, timedelta\n",
    "#from gzip import decompress\n",
    "from json import loads\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "#from getpass import getpass\n",
    "from collections import defaultdict\n",
    "import base64\n",
    "import twitter\n",
    "from twitter import *\n",
    "#from bs4 import BeautifulSoup as bs\n",
    "import emoji\n",
    "#from emot.emo_unicode import EMOTICONS\n",
    "import re\n",
    "from string import punctuation\n",
    "import random\n",
    "#import pickle\n",
    "from Apikey import *\n",
    "import nltk\n",
    "from nltk.corpus import treebank, stopwords, movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import ClassifierI\n",
    "from nltk.tokenize import word_tokenize\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from bson.json_util import dumps\n",
    "password = 'Fluma011'\n",
    "\n",
    "#from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "#from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "#from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "#from statistics import mode\n",
    "#password = 'Fluma011'\n",
    "\n",
    "print(emoji.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Connecting to Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_secret = '{}:{}'.format(token, token_secret).encode('ascii')\n",
    "b64_encoded_key = base64.b64encode(key_secret)\n",
    "b64_encoded_key = b64_encoded_key.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://api.twitter.com/'\n",
    "auth_url = '{}oauth2/token'.format(base_url)\n",
    "\n",
    "auth_headers = {\n",
    "    'Authorization': 'Basic {}'.format(b64_encoded_key),\n",
    "    'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'\n",
    "}\n",
    "\n",
    "auth_data = {\n",
    "    'grant_type': 'client_credentials'\n",
    "}\n",
    "\n",
    "auth_resp = requests.post(auth_url, headers=auth_headers, data=auth_data)\n",
    "auth_resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_type', 'access_token'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keys in data response are token_type (bearer) and access_token (your access token)\n",
    "auth_resp.json().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = auth_resp.json()['access_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Pulling Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjects to Analyze:\n",
    "\n",
    "1. \"organic supplements\" OR \"vegan supplements\" - Vegan/Organic food\n",
    "2. @BioTrust and @SpekterLabs - Competitors\n",
    "3. all natural pre workout\n",
    "4. brain enhancement supplements\n",
    "5. health and wellness products online\n",
    "6. healthiest pre workout\n",
    "7. immune support supplements\n",
    "8. supplements to reduce stress\n",
    "9. supplements for work\n",
    "10. immune system supplements\n",
    "11. meal replacement powder\n",
    "12. meal replacement protein powder\n",
    "13. mental calm supplement\n",
    "14. natural pre workout\n",
    "15. natural workout supplements\n",
    "16. nootropics for creativity\n",
    "17. nootropics for sale\n",
    "18. organic meal replacement\n",
    "19. protein meal replacement\n",
    "20. protein powder meal replacement\n",
    "21. relaxation supplements\n",
    "22. stress relief supplements\n",
    "23. supplement for endurance\n",
    "24. supplements for focus and concentration\n",
    "25. supplements for immune system\n",
    "26. supplements for stress\n",
    "27. supplements for stress and fatigue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_word = ['\"organic supplements\" OR \"vegan supplements\"',\n",
    "               '\"BioTrust\" OR \"SpekterLabs\"',\n",
    "               '\"all natural pre workout\" OR \"healthiest pre workout\" OR \"natural pre workout\" OR \"natural workout supplements\"',\n",
    "               '\"meal replacement powder\" OR \"meal replacement protein powder\" OR \"organic meal replacement\" OR \"protein meal replacement\" OR \"protein powder meal replacement\"',\n",
    "               '\"mental calm supplement\" OR \"relaxation supplements\" OR \"stress relief supplements\" OR \"supplements for stress\" OR \"supplements for stress and fatigue\" OR \"supplements to reduce stress\"',\n",
    "               '\"health and wellness products online\" OR \"immune support supplements\" OR \"immune system supplements\" OR \"supplement for endurance\" OR \"supplements for immune system\"',\n",
    "               '\"brain enhancement supplements\" OR \"nootropics for creativity\" OR \"supplements for focus and concentration\"',\n",
    "               '\"supplements for work\" OR \"nootropics for sale\"']\n",
    "len(search_word)\n",
    "# saved parameter: \"organic supplements\" OR \"vegan supplements\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=1\n",
    "for xparams in search_word:\n",
    "    search_headers = {\n",
    "    'Authorization': 'Bearer {}'.format(access_token)    \n",
    "    }\n",
    "    search_params = {\n",
    "        'q': xparams, #subject search parameter\n",
    "        'until': '2020-12-14', #date parameter available for last 7 days\n",
    "        'result_type': 'recent', #type of result: most recent tweets\n",
    "        'tweet_mode':'extended', #full text of the tweet\n",
    "        'count': 100, #number of tweets to query\n",
    "        'lang':'en' #language of tweets\n",
    "    }\n",
    "    search_url = '{}1.1/search/tweets.json'.format(base_url)\n",
    "    search_resp = requests.get(search_url, headers=search_headers, params=search_params)\n",
    "    globals()['tweet_data_%s'%x] = search_resp.json()\n",
    "    globals()['tweet_data_name_%s'%x] = xparams\n",
    "    x=x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': 'Sun Dec 13 17:37:59 +0000 2020', 'id': 1338176352541880330, 'id_str': '1338176352541880330', 'full_text': \"@TropicanaHaze @FrasierHarry Lol! Actually, it's less work - hemp protein can be bought in power form and is easily mixed with some warmed almond milk - salads are a 'toss' and corn cakes &amp; wheat-free bread all available in super-markets - too busy to have time for cooking. I do take vegan supplements too üåà\", 'truncated': False, 'display_text_range': [29, 312], 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'TropicanaHaze', 'name': 'Da Fifth Element ü¶ã‚ú®üêâ', 'id': 22030326, 'id_str': '22030326', 'indices': [0, 14]}, {'screen_name': 'FrasierHarry', 'name': 'Harry Frasier', 'id': 4809044939, 'id_str': '4809044939', 'indices': [15, 28]}], 'urls': []}, 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'}, 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>', 'in_reply_to_status_id': 1338173843471556609, 'in_reply_to_status_id_str': '1338173843471556609', 'in_reply_to_user_id': 22030326, 'in_reply_to_user_id_str': '22030326', 'in_reply_to_screen_name': 'TropicanaHaze', 'user': {'id': 1176853129121476608, 'id_str': '1176853129121476608', 'name': 'Deborah Mahmoudieh', 'screen_name': 'DeborahMahmoud1', 'location': '', 'description': 'Vegan, Human Rights, #Corbyn, #Remain, Ecology, Child Welfare... ANYONE ADDING MY NAME TO LISTS WILL BE INSTANTLY BLOCKED.', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1231, 'friends_count': 1552, 'listed_count': 1, 'created_at': 'Wed Sep 25 13:38:37 +0000 2019', 'favourites_count': 26282, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 32025, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1176854014488694784/4Wsm0a3C_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1176854014488694784/4Wsm0a3C_normal.jpg', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 1, 'favorited': False, 'retweeted': False, 'lang': 'en'}\n"
     ]
    }
   ],
   "source": [
    "search_resp.json().keys()\n",
    "#print(tweet_data_1['statuses'][0].keys(),'\\n')\n",
    "print(tweet_data_1['statuses'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Sun Dec 13 17:37:59 +0000 2020'\n",
      "'Sun Dec 13 16:19:59 +0000 2020'\n",
      "'Sun Dec 13 14:09:51 +0000 2020'\n",
      "'Sun Dec 13 02:25:29 +0000 2020'\n",
      "'Sat Dec 12 19:57:40 +0000 2020'\n",
      "'Sat Dec 12 19:46:24 +0000 2020'\n",
      "'Sat Dec 12 14:35:56 +0000 2020'\n",
      "'Sat Dec 12 14:32:35 +0000 2020'\n",
      "'Sat Dec 12 06:58:32 +0000 2020'\n",
      "'Fri Dec 11 21:21:19 +0000 2020'\n",
      "'Fri Dec 11 21:15:12 +0000 2020'\n",
      "'Fri Dec 11 16:07:32 +0000 2020'\n",
      "'Fri Dec 11 14:25:28 +0000 2020'\n",
      "'Fri Dec 11 12:00:18 +0000 2020'\n",
      "'Thu Dec 10 15:59:04 +0000 2020'\n",
      "'Thu Dec 10 15:50:18 +0000 2020'\n",
      "'Thu Dec 10 15:44:18 +0000 2020'\n",
      "'Thu Dec 10 13:33:22 +0000 2020'\n",
      "'Thu Dec 10 11:08:35 +0000 2020'\n",
      "'Thu Dec 10 02:40:42 +0000 2020'\n",
      "'Thu Dec 10 02:25:18 +0000 2020'\n",
      "'Thu Dec 10 01:28:20 +0000 2020'\n",
      "'Wed Dec 09 18:55:20 +0000 2020'\n",
      "'Wed Dec 09 08:09:09 +0000 2020'\n",
      "'Wed Dec 09 05:18:00 +0000 2020'\n",
      "'Wed Dec 09 04:30:35 +0000 2020'\n",
      "'Wed Dec 09 04:30:35 +0000 2020'\n",
      "'Wed Dec 09 02:37:08 +0000 2020'\n",
      "'Wed Dec 09 00:32:13 +0000 2020'\n",
      "'Tue Dec 08 14:25:58 +0000 2020'\n",
      "'Tue Dec 08 12:43:42 +0000 2020'\n",
      "'Tue Dec 08 05:38:39 +0000 2020'\n",
      "'Tue Dec 08 05:08:54 +0000 2020'\n",
      "'Tue Dec 08 01:25:53 +0000 2020'\n",
      "'Mon Dec 07 20:36:08 +0000 2020'\n",
      "'Mon Dec 07 18:54:58 +0000 2020'\n",
      "'Mon Dec 07 14:02:04 +0000 2020'\n",
      "'Mon Dec 07 05:08:59 +0000 2020'\n",
      "'Mon Dec 07 04:38:57 +0000 2020'\n",
      "'Mon Dec 07 03:10:20 +0000 2020'\n",
      "'Mon Dec 07 02:25:10 +0000 2020'\n",
      "'Mon Dec 07 01:04:29 +0000 2020'\n",
      "'Mon Dec 07 01:03:08 +0000 2020'\n",
      "Total tweets: 43 \n",
      "Collection: 1 \"organic supplements\" OR \"vegan supplements\" \n",
      "\n",
      "'Sat Dec 12 21:29:02 +0000 2020'\n",
      "'Sat Dec 12 19:49:43 +0000 2020'\n",
      "'Fri Dec 11 23:31:09 +0000 2020'\n",
      "'Fri Dec 11 15:17:52 +0000 2020'\n",
      "'Fri Dec 11 05:43:54 +0000 2020'\n",
      "'Fri Dec 11 05:43:44 +0000 2020'\n",
      "'Thu Dec 10 17:11:32 +0000 2020'\n",
      "'Thu Dec 10 15:25:35 +0000 2020'\n",
      "'Thu Dec 10 15:25:13 +0000 2020'\n",
      "'Thu Dec 10 15:25:02 +0000 2020'\n",
      "'Wed Dec 09 18:33:28 +0000 2020'\n",
      "'Tue Dec 08 18:48:14 +0000 2020'\n",
      "'Mon Dec 07 15:46:40 +0000 2020'\n",
      "Total tweets: 13 \n",
      "Collection: 2 \"BioTrust\" OR \"SpekterLabs\" \n",
      "\n",
      "'Sun Dec 13 17:55:00 +0000 2020'\n",
      "'Sun Dec 13 11:03:07 +0000 2020'\n",
      "'Sun Dec 13 10:56:16 +0000 2020'\n",
      "'Sat Dec 12 00:38:00 +0000 2020'\n",
      "'Thu Dec 10 14:42:13 +0000 2020'\n",
      "'Thu Dec 10 03:40:02 +0000 2020'\n",
      "'Wed Dec 09 16:28:52 +0000 2020'\n",
      "'Tue Dec 08 18:00:53 +0000 2020'\n",
      "'Mon Dec 07 19:50:01 +0000 2020'\n",
      "'Mon Dec 07 14:52:37 +0000 2020'\n",
      "Total tweets: 10 \n",
      "Collection: 3 \"all natural pre workout\" OR \"healthiest pre workout\" OR \"natural pre workout\" OR \"natural workout supplements\" \n",
      "\n",
      "'Sun Dec 13 18:20:36 +0000 2020'\n",
      "'Sun Dec 13 13:03:12 +0000 2020'\n",
      "'Sun Dec 13 12:56:22 +0000 2020'\n",
      "'Fri Dec 11 18:52:41 +0000 2020'\n",
      "'Fri Dec 11 03:58:35 +0000 2020'\n",
      "'Thu Dec 10 09:51:16 +0000 2020'\n",
      "'Wed Dec 09 20:34:13 +0000 2020'\n",
      "'Wed Dec 09 13:04:25 +0000 2020'\n",
      "'Wed Dec 09 09:34:29 +0000 2020'\n",
      "'Wed Dec 09 05:28:49 +0000 2020'\n",
      "'Wed Dec 09 04:15:55 +0000 2020'\n",
      "'Tue Dec 08 22:42:42 +0000 2020'\n",
      "'Tue Dec 08 18:00:26 +0000 2020'\n",
      "'Tue Dec 08 03:51:44 +0000 2020'\n",
      "'Mon Dec 07 17:12:01 +0000 2020'\n",
      "'Mon Dec 07 11:21:42 +0000 2020'\n",
      "'Mon Dec 07 09:54:26 +0000 2020'\n",
      "'Mon Dec 07 08:32:29 +0000 2020'\n",
      "'Mon Dec 07 08:00:04 +0000 2020'\n",
      "'Mon Dec 07 06:54:14 +0000 2020'\n",
      "Total tweets: 20 \n",
      "Collection: 4 \"meal replacement powder\" OR \"meal replacement protein powder\" OR \"organic meal replacement\" OR \"protein meal replacement\" OR \"protein powder meal replacement\" \n",
      "\n",
      "'Fri Dec 11 11:02:03 +0000 2020'\n",
      "'Wed Dec 09 14:34:28 +0000 2020'\n",
      "'Tue Dec 08 16:06:20 +0000 2020'\n",
      "'Tue Dec 08 09:33:01 +0000 2020'\n",
      "Total tweets: 4 \n",
      "Collection: 5 \"mental calm supplement\" OR \"relaxation supplements\" OR \"stress relief supplements\" OR \"supplements for stress\" OR \"supplements for stress and fatigue\" OR \"supplements to reduce stress\" \n",
      "\n",
      "'Sun Dec 13 15:29:36 +0000 2020'\n",
      "'Sun Dec 13 15:00:42 +0000 2020'\n",
      "'Sun Dec 13 02:47:26 +0000 2020'\n",
      "'Sat Dec 12 18:22:19 +0000 2020'\n",
      "'Thu Dec 10 18:25:03 +0000 2020'\n",
      "'Thu Dec 10 18:06:54 +0000 2020'\n",
      "'Thu Dec 10 12:05:26 +0000 2020'\n",
      "'Thu Dec 10 01:35:19 +0000 2020'\n",
      "'Wed Dec 09 19:26:31 +0000 2020'\n",
      "'Tue Dec 08 23:22:48 +0000 2020'\n",
      "'Tue Dec 08 11:23:27 +0000 2020'\n",
      "'Tue Dec 08 04:00:43 +0000 2020'\n",
      "'Tue Dec 08 01:34:05 +0000 2020'\n",
      "'Mon Dec 07 23:00:07 +0000 2020'\n",
      "'Mon Dec 07 10:26:05 +0000 2020'\n",
      "'Sun Dec 06 14:57:08 +0000 2020'\n",
      "Total tweets: 16 \n",
      "Collection: 6 \"health and wellness products online\" OR \"immune support supplements\" OR \"immune system supplements\" OR \"supplement for endurance\" OR \"supplements for immune system\" \n",
      "\n",
      "Total tweets: 0 \n",
      "Collection: 7 \"brain enhancement supplements\" OR \"nootropics for creativity\" OR \"supplements for focus and concentration\" \n",
      "\n",
      "Total tweets: 0 \n",
      "Collection: 8 \"supplements for work\" OR \"nootropics for sale\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y=1\n",
    "while y <= len(search_word):\n",
    "    for x in globals()['tweet_data_%s'%y]['statuses']:\n",
    "        pprint.pprint(x['created_at'])\n",
    "    print('Total tweets:', len(globals()['tweet_data_%s'%y]['statuses']),'\\nCollection:',y,globals()['tweet_data_name_%s'%y],'\\n')\n",
    "    y=y+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For retweets, retrieve the full tweet\n",
    "\n",
    "Create a list to store the attributes to analyze: Tweet ID, Date Created, Tweet, Username, Retweet Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict_keys(['created_at', 'id', 'id_str', 'full_text', 'truncated', 'display_text_range', 'entities', 'extended_entities', 'metadata', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'lang']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': 1338144046431301632,\n",
       " 'id_str': '1338144046431301632',\n",
       " 'Truncated': False,\n",
       " 'Display_text_range': [0, 159],\n",
       " 'Entites': {'hashtags': [{'text': 'trynatural', 'indices': [124, 135]}],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [{'screen_name': 'sambucolusa',\n",
       "    'name': 'Sambucol USA',\n",
       "    'id': 65417385,\n",
       "    'id_str': '65417385',\n",
       "    'indices': [44, 56]},\n",
       "   {'screen_name': 'SocialNature',\n",
       "    'name': 'Social Nature',\n",
       "    'id': 88011228,\n",
       "    'id_str': '88011228',\n",
       "    'indices': [99, 112]}],\n",
       "  'urls': [{'url': 'https://t.co/qGSmLPDlK5',\n",
       "    'expanded_url': 'https://www.socialnature.com/l-pharmacare-immune-support-supplements?social=twitter&user_referrer=43133&user_referral_channel=twitter&product=542',\n",
       "    'display_url': 'socialnature.com/l-pharmacare-i‚Ä¶',\n",
       "    'indices': [136, 159]}]},\n",
       " 'Metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
       " 'Source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       " 'In_reply_to_status_id': None,\n",
       " 'In_reply_to_status_id_str': None,\n",
       " 'In_reply_to_user_id': None,\n",
       " 'In_reply_to_user_id_str': None,\n",
       " 'In_reply_to_screen_name': None,\n",
       " 'User': {'id': 1340922602,\n",
       "  'id_str': '1340922602',\n",
       "  'name': 'Angie Baby',\n",
       "  'screen_name': 'lilliansgifts',\n",
       "  'location': 'United States',\n",
       "  'description': 'We have a little something for everyone!! We give great customer service and stand behind our products that we offer, from home decor to unique gift ideas!!',\n",
       "  'url': 'https://t.co/A9J5lXE7iR',\n",
       "  'entities': {'url': {'urls': [{'url': 'https://t.co/A9J5lXE7iR',\n",
       "      'expanded_url': 'http://www.lilliansgifts.com',\n",
       "      'display_url': 'lilliansgifts.com',\n",
       "      'indices': [0, 23]}]},\n",
       "   'description': {'urls': []}},\n",
       "  'protected': False,\n",
       "  'followers_count': 616,\n",
       "  'friends_count': 162,\n",
       "  'listed_count': 4,\n",
       "  'created_at': 'Wed Apr 10 03:41:18 +0000 2013',\n",
       "  'favourites_count': 2114,\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': False,\n",
       "  'verified': False,\n",
       "  'statuses_count': 6180,\n",
       "  'lang': None,\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_background_color': '642D8B',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme10/bg.gif',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme10/bg.gif',\n",
       "  'profile_background_tile': True,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/3500849487/934a3e27a3f0fe52d523e54ce05f51cf_normal.jpeg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/3500849487/934a3e27a3f0fe52d523e54ce05f51cf_normal.jpeg',\n",
       "  'profile_link_color': 'FF0000',\n",
       "  'profile_sidebar_border_color': '65B0DA',\n",
       "  'profile_sidebar_fill_color': '7AC3EE',\n",
       "  'profile_text_color': '3D1957',\n",
       "  'profile_use_background_image': True,\n",
       "  'has_extended_profile': True,\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'following': None,\n",
       "  'follow_request_sent': None,\n",
       "  'notifications': None,\n",
       "  'translator_type': 'none'},\n",
       " 'Geo': None,\n",
       " 'Coordinates': None,\n",
       " 'Place': None,\n",
       " 'Contributors': None,\n",
       " 'is_quote_status': False,\n",
       " 'retweeted': False,\n",
       " 'lang': 'en',\n",
       " 'CreatedDate': 'Sun Dec 13 15:29:36 +0000 2020',\n",
       " 'Tweet': \"I'm so excited about Immune Supplement from @sambucolusa! Help bring them to a store near you with @socialnature Launchpad! #trynatural https://t.co/qGSmLPDlK5\",\n",
       " 'UserName': 'Angie Baby',\n",
       " 'Retweet_Count': 0,\n",
       " 'extended_entities': 0,\n",
       " 'possibly_sensitive': False,\n",
       " 'favorite_count': 0,\n",
       " 'favorited': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=1\n",
    "while a <= len(search_word):\n",
    "    globals()['tweetlist_%s'%a] = []\n",
    "    a=a+1\n",
    "\n",
    "y=1\n",
    "while y <= len(search_word):\n",
    "    for x in globals()['tweet_data_%s'%y]['statuses']:\n",
    "        if 'retweeted_status' in x.keys(): \n",
    "            full_text = x['retweeted_status']['full_text']\n",
    "        else:\n",
    "            full_text = x['full_text']\n",
    "        if 'extended_entities' in x.keys(): \n",
    "            extended_entities = x['extended_entities']\n",
    "        else:\n",
    "            extended_entities = 0\n",
    "        if 'possibly_sensitive' in x.keys(): \n",
    "            possibly_sensitive = x['possibly_sensitive']\n",
    "        else:\n",
    "            possibly_sensitive = False\n",
    "        globals()['tweetlist_%s'%y].append({  '_id':x['id'],\n",
    "                                              'id_str':x['id_str'],\n",
    "                                              'Truncated':x['truncated'],\n",
    "                                              'Display_text_range':x['display_text_range'],\n",
    "                                              'Entites':x['entities'],\n",
    "                                              'Metadata':x['metadata'],\n",
    "                                              'Source':x['source'],\n",
    "                                              'In_reply_to_status_id':x['in_reply_to_status_id'],\n",
    "                                              'In_reply_to_status_id_str':x['in_reply_to_status_id_str'],\n",
    "                                              'In_reply_to_user_id':x['in_reply_to_user_id'],\n",
    "                                              'In_reply_to_user_id_str':x['in_reply_to_user_id_str'],\n",
    "                                              'In_reply_to_screen_name':x['in_reply_to_screen_name'],\n",
    "                                              'User':x['user'],\n",
    "                                              'Geo':x['geo'],\n",
    "                                              'Coordinates':x['coordinates'],\n",
    "                                              'Place':x['place'],\n",
    "                                              'Contributors':x['contributors'],\n",
    "                                              'is_quote_status':x['is_quote_status'],\n",
    "                                              'retweeted':x['retweeted'],\n",
    "                                              'lang':x['lang'],\n",
    "                                              'CreatedDate':x['created_at'],\n",
    "                                              'Tweet':full_text,\n",
    "                                              'UserName':x['user']['name'],\n",
    "                                              'Retweet_Count':x['retweet_count'],\n",
    "                                              'extended_entities':extended_entities,\n",
    "                                              'possibly_sensitive':possibly_sensitive,\n",
    "                                              'favorite_count':x['favorite_count'],\n",
    "                                              'favorited':x['favorited']})\n",
    "    y=y+1\n",
    "tweetlist_6[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Cleaning Data & Removing Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = re.compile(r'#https:+//+\\S+|,|!|\\?|:|\\.|;|&|-|_|\"|\\'|@+\\w+|\\n')\n",
    "\n",
    "def demojify(txt):\n",
    "    return(emoji.get_emoji_regexp().sub(u'', txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "while i <= len(search_word):\n",
    "    for index in globals()['tweetlist_%s'%i]:\n",
    "        index['Tweet'] = demojify(re.sub(scrape,'',(index['Tweet']).lower()))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Pushing Data to Cloud MongoDB Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=MongoClient('mongodb+srv://andre:'+password+'@clustera-lifo7.azure.mongodb.net/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client=MongoClient('mongodb+srv://littlethings123:'+'littlethings123'+'@cluster0.89muq.azure.mongodb.net/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Potter', 'Thrones', 'Thrones2', 'Weather', 'groubydb', 'mydatabase', 'namedb', 'test', 'testdb', 'twitterdb', 'wholepreneurs_db', 'admin', 'local']\n"
     ]
    }
   ],
   "source": [
    "print(client.list_database_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wholepreneurs_db = client.wholepreneurs_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collections:\n",
    "\n",
    "1. Vegan_Organic_Collection - Collection of tweets about Vegan/Organic food (\"organic supplements\" OR \"vegan supplements\")\n",
    "2. Competitors_Collection - Collection of tweets about Competitors (@BioTrust and @SpekterLabs)\n",
    "3. Workout_Collection - Collection of tweets about Workout food (all natural pre workout | healthiest pre workout | natural pre workout | natural workout supplements)\n",
    "4. Meal_Replacement_Collection - Collection of tweets about Meal Replacement (meal replacement powder | meal replacement protein powder | organic meal replacement | protein meal replacement | protein powder meal replacement)\n",
    "5. Stress_Relief_Collection - Collection of tweets about Stress relief and calm (mental calm supplement | relaxation supplements | stress relief supplements | supplements for stress | supplements for stress and fatigue | supplements to reduce stress)\n",
    "6. Health_Collection - Collection of tweets about Health (health and wellness products online | immune support supplements | immune system supplements | supplement for endurance | supplements for immune system)\n",
    "7. Brain_Enhancement_Food_Collection - Collection of tweets about Food for enhancing mental power (brain enhancement supplements | nootropics for creativity | supplements for focus and concentration)\n",
    "8. Others_Collection - Collection ot tweets about other subjects (supplements for work | nootropics for sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pymongo.cursor.Cursor'>\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "idx = 1328813887437856782\n",
    "\n",
    "print(type(client['wholepreneurs_db']['Vegan_Organic_Collection'].find({'_id':idx},{\"_id\":1})))\n",
    "doc = client['wholepreneurs_db']['Vegan_Organic_Collection'].find_one({'_id':idx})\n",
    "if  doc != None:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vegan_Organic_Collection = Wholepreneurs_db.Vegan_Organic_Collection\n",
    "Competitors_Collection  = Wholepreneurs_db.Competitors_Collection\n",
    "Workout_Collection = Wholepreneurs_db.Workout_Collection\n",
    "Meal_Replacement_Collection = Wholepreneurs_db.Meal_Replacement_Collection\n",
    "Stress_Relief_Collection = Wholepreneurs_db.Stress_Relief_Collection\n",
    "Health_Collection = Wholepreneurs_db.Health_Collection\n",
    "Brain_Enhancement_Food_Collection = Wholepreneurs_db.Brain_Enhancement_Food_Collection\n",
    "Others_Collection = Wholepreneurs_db.Others_Collection\n",
    "Sentiment_Collection = Wholepreneurs_db.Sentiment_Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 1\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 2\n",
      "Record already inserted for Collection 3\n",
      "Record already inserted for Collection 3\n",
      "Record already inserted for Collection 3\n",
      "Record already inserted for Collection 3\n",
      "Record already inserted for Collection 3\n",
      "Record already inserted for Collection 3\n",
      "Record already inserted for Collection 3\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 4\n",
      "Record already inserted for Collection 5\n",
      "Record already inserted for Collection 5\n",
      "Record already inserted for Collection 5\n",
      "Record already inserted for Collection 5\n",
      "Record already inserted for Collection 6\n",
      "Record already inserted for Collection 6\n",
      "Record already inserted for Collection 6\n",
      "Record already inserted for Collection 6\n",
      "Record already inserted for Collection 6\n",
      "Record already inserted for Collection 6\n",
      "Record already inserted for Collection 6\n",
      "Record already inserted for Collection 6\n",
      "Record already inserted for Collection 6\n",
      "Record already inserted for Collection 6\n",
      "Record already inserted for Collection 6\n",
      "Record already inserted for Collection 6\n",
      "Record already inserted for Collection 6\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while i <= len(search_word):\n",
    "    for data in globals()['tweetlist_%s'%i]:\n",
    "        idx = data['_id']\n",
    "        if i == 1:\n",
    "            check = Vegan_Organic_Collection.find_one({'_id':idx})\n",
    "            if check != None:\n",
    "                print('Record already inserted for Collection 1')\n",
    "            else:\n",
    "                Vegan_Organic_Collection.insert_one(data)\n",
    "        elif i == 2:\n",
    "            check = Competitors_Collection.find_one({'_id':idx})\n",
    "            if check != None:\n",
    "                print('Record already inserted for Collection 2')\n",
    "            else:\n",
    "                Competitors_Collection.insert_one(data)\n",
    "        elif i == 3:\n",
    "            check = Workout_Collection.find_one({'_id':idx})\n",
    "            if check != None:\n",
    "                print('Record already inserted for Collection 3')\n",
    "            else:\n",
    "                Workout_Collection.insert_one(data)\n",
    "        elif i == 4:\n",
    "            check = Meal_Replacement_Collection.find_one({'_id':idx})\n",
    "            if check != None:\n",
    "                print('Record already inserted for Collection 4')\n",
    "            else:\n",
    "                Meal_Replacement_Collection.insert_one(data)\n",
    "        elif i == 5:\n",
    "            check = Stress_Relief_Collection.find_one({'_id':idx})\n",
    "            if check != None:\n",
    "                print('Record already inserted for Collection 5')\n",
    "            else:\n",
    "                Stress_Relief_Collection.insert_one(data)\n",
    "        elif i == 6:\n",
    "            check = Health_Collection.find_one({'_id':idx})\n",
    "            if check != None:\n",
    "                print('Record already inserted for Collection 6')\n",
    "            else:\n",
    "                Health_Collection.insert_one(data)\n",
    "        elif i == 7:\n",
    "            check = Brain_Enhancement_Food_Collection.find_one({'_id':idx})\n",
    "            if check != None:\n",
    "                print('Record already inserted for Collection 7')\n",
    "            else:\n",
    "                Brain_Enhancement_Food_Collection.insert_one(data)\n",
    "        else:\n",
    "            check = Others_Collection.find_one({'_id':idx})\n",
    "            if check != None:\n",
    "                print('Record already inserted for Collection 8')\n",
    "            else:\n",
    "                Others_Collection.insert_one(data)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of commonly used words (stop words), punctuations, special characters and inflections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + list(punctuation) + ['--', '``', \"'s\", \"n't\", \"'re\", \"'m\",\"'ll\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training dataset using prescreened positive and negative tweets and adjectives\n",
    "\n",
    "Clean the training lists and assign category for each text (positive or negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/larissa/desktop/canada/langara/2nd_term/CPSC_4810/Group Project/Training Model/'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_short_pos_1 = open(path+\"positive-words.txt\",encoding='latin-1').read()\n",
    "list_short_neg_1 = open(path+\"negative-words.txt\",encoding='latin-1').read()\n",
    "\n",
    "list_short_pos_2 = open(path+\"positive.txt\",encoding='latin-1').read()\n",
    "list_short_neg_2 = open(path+\"negative.txt\",encoding='latin-1').read()\n",
    "\n",
    "short_pos = list_short_pos_1 + list_short_pos_2\n",
    "short_neg = list_short_neg_1 + list_short_neg_2\n",
    "\n",
    "documents = []\n",
    "\n",
    "for r in short_pos.split('\\n'):\n",
    "    r = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', r) # remove URLs\n",
    "    r = r.replace(\"‚Äô\", \" \")\n",
    "    r = r.replace(\"``\", \" \")\n",
    "    r = r.replace('\"', \" \")\n",
    "    r = re.sub(r'#/,([^\\s]+)', r'\\1', r) # remove the # in #hashtag\n",
    "    documents.append( (r, \"pos\") )\n",
    "\n",
    "for r in short_neg.split('\\n'):\n",
    "    r = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', r) # remove URLs\n",
    "    r = r.replace(\"‚Äô\", \" \")\n",
    "    r = r.replace(\"``\", \" \")\n",
    "    r = r.replace('\"', \" \")\n",
    "    r = re.sub(r'#/,([^\\s]+)', r'\\1', r) # remove the # in #hashtag\n",
    "    documents.append( (r, \"neg\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of all words in the dataset (excluding stopwords) using the word tokenize function\n",
    "\n",
    "Create a dictionary with the words as key and the frequency as value\n",
    "\n",
    "Create a list of 5000 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "\n",
    "short_pos_words = nltk.word_tokenize(short_pos)\n",
    "short_neg_words = nltk.word_tokenize(short_neg)\n",
    "\n",
    "for word in short_pos_words:\n",
    "    if word not in stop_words:\n",
    "        all_words.append(word.lower()) \n",
    "\n",
    "for word in short_neg_words:\n",
    "    if word not in stop_words:\n",
    "        all_words.append(word.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use user defined function to go through each word in the tweet and assign True or False depending on if it is present in the dictionary\n",
    "\n",
    "Randomize the resulting list to reduce bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a+', 'abound', 'abounds']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFeatures = []\n",
    "def find_features(document):\n",
    "    words = nltk.word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "trainingFeatures = [(find_features(testing_tweet), category) for (testing_tweet, category) in documents]\n",
    "random.shuffle(trainingFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Naive Bayes Classifier to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBayesClassifier = nltk.NaiveBayesClassifier.train(trainingFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyzing the subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained model to assign a sentiment to each tweet of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vegan_Organic: 158\n",
      "Competitors: 38\n",
      "Workout: 13\n",
      "Meal_Replacement: 111\n",
      "Stress_Relief: 15\n",
      "Health: 70\n",
      "Brain_Enhancement: 2\n",
      "Others: 1\n",
      "Total: 408\n"
     ]
    }
   ],
   "source": [
    "#==\n",
    "NBResultLabels1 = [NBayesClassifier.classify(find_features(data['Tweet'])) for data in Vegan_Organic_Collection.find()]\n",
    "print('Vegan_Organic:',len(NBResultLabels1))\n",
    "\n",
    "#==\n",
    "NBResultLabels2 = [NBayesClassifier.classify(find_features(data['Tweet'])) for data in Competitors_Collection.find()]\n",
    "print('Competitors:',len(NBResultLabels2))\n",
    "\n",
    "#==\n",
    "NBResultLabels3 = [NBayesClassifier.classify(find_features(data['Tweet'])) for data in Workout_Collection.find()]\n",
    "print('Workout:',len(NBResultLabels3))\n",
    "\n",
    "#==\n",
    "NBResultLabels4 = [NBayesClassifier.classify(find_features(data['Tweet'])) for data in Meal_Replacement_Collection.find()]\n",
    "print('Meal_Replacement:',len(NBResultLabels4))\n",
    "\n",
    "#==\n",
    "NBResultLabels5 = [NBayesClassifier.classify(find_features(data['Tweet'])) for data in Stress_Relief_Collection.find()]\n",
    "print('Stress_Relief:',len(NBResultLabels5))\n",
    "\n",
    "#==\n",
    "NBResultLabels6 = [NBayesClassifier.classify(find_features(data['Tweet'])) for data in Health_Collection.find()]\n",
    "print('Health:',len(NBResultLabels6))\n",
    "\n",
    "#==\n",
    "NBResultLabels7 = [NBayesClassifier.classify(find_features(data['Tweet'])) for data in Brain_Enhancement_Food_Collection.find()]\n",
    "print('Brain_Enhancement:',len(NBResultLabels7))\n",
    "\n",
    "#==\n",
    "NBResultLabels8 = [NBayesClassifier.classify(find_features(data['Tweet'])) for data in Others_Collection.find()]\n",
    "print('Others:',len(NBResultLabels8))\n",
    "\n",
    "#==\n",
    "print('Total:',(len(NBResultLabels1)+len(NBResultLabels2)+len(NBResultLabels3)+\n",
    "                len(NBResultLabels4)+len(NBResultLabels5)+len(NBResultLabels6)+\n",
    "                len(NBResultLabels7)+len(NBResultLabels8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larissa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "  \n",
      "/Users/larissa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/larissa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "  app.launch_new_instance()\n",
      "/Users/larissa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "/Users/larissa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "/Users/larissa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "/Users/larissa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "/Users/larissa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:41: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n"
     ]
    }
   ],
   "source": [
    "tweet_text = []\n",
    "\n",
    "w = 1\n",
    "while w <= len(search_word):\n",
    "    \n",
    "    for i in range(Vegan_Organic_Collection.count()):\n",
    "        tweet_text.append(Vegan_Organic_Collection.find()[i][\"_id\"])\n",
    "    df1 = pd.DataFrame(zip(tweet_text,NBResultLabels1),columns = ['_id','Sentiment'])\n",
    "    tweet_text = []\n",
    "    \n",
    "    for i in range(Competitors_Collection.count()):\n",
    "        tweet_text.append(Competitors_Collection.find()[i][\"_id\"])\n",
    "    df2 = pd.DataFrame(zip(tweet_text,NBResultLabels2),columns = ['_id','Sentiment'])\n",
    "    tweet_text = []\n",
    "    \n",
    "    for i in range(Workout_Collection.count()):\n",
    "        tweet_text.append(Workout_Collection.find()[i][\"_id\"])\n",
    "    df3 = pd.DataFrame(zip(tweet_text,NBResultLabels3),columns = ['_id','Sentiment'])\n",
    "    tweet_text = []\n",
    "    \n",
    "    for i in range(Meal_Replacement_Collection.count()):\n",
    "        tweet_text.append(Meal_Replacement_Collection.find()[i][\"_id\"])   \n",
    "    df4 = pd.DataFrame(zip(tweet_text,NBResultLabels4),columns = ['_id','Sentiment'])\n",
    "    tweet_text = []\n",
    "    \n",
    "    for i in range(Stress_Relief_Collection.count()):\n",
    "        tweet_text.append(Stress_Relief_Collection.find()[i][\"_id\"])    \n",
    "    df5 = pd.DataFrame(zip(tweet_text,NBResultLabels5),columns = ['_id','Sentiment'])\n",
    "    tweet_text = []\n",
    "    \n",
    "    for i in range(Health_Collection.count()):\n",
    "        tweet_text.append(Health_Collection.find()[i][\"_id\"])    \n",
    "    df6 = pd.DataFrame(zip(tweet_text,NBResultLabels6),columns = ['_id','Sentiment'])\n",
    "    tweet_text = []\n",
    "    \n",
    "    for i in range(Brain_Enhancement_Food_Collection.count()):\n",
    "        tweet_text.append(Brain_Enhancement_Food_Collection.find()[i][\"_id\"])    \n",
    "    df7 = pd.DataFrame(zip(tweet_text,NBResultLabels7),columns = ['_id','Sentiment'])\n",
    "    tweet_text = []\n",
    "    \n",
    "    for i in range(Others_Collection.count()):\n",
    "        tweet_text.append(Others_Collection.find()[i][\"_id\"])\n",
    "    df8 = pd.DataFrame(zip(tweet_text,NBResultLabels8),columns = ['_id','Sentiment'])\n",
    "    tweet_text = []\n",
    "    \n",
    "    w=w+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_excel(\"~/Desktop/Canada/Langara/3rd_term/CPSC_4820/Group_Project/sentiment.xlsx\",sheet_name=\"Vegan_Organic\")\n",
    "with pd.ExcelWriter(\"sentiment.xlsx\", engine=\"openpyxl\", mode='a') as writer:\n",
    "    df2.to_excel(writer,sheet_name=\"Competitors\")\n",
    "    df3.to_excel(writer,sheet_name=\"Workout\")\n",
    "    df4.to_excel(writer,sheet_name=\"Meal_Replacement\")\n",
    "    df5.to_excel(writer,sheet_name=\"Stress_Relief\")\n",
    "    df6.to_excel(writer,sheet_name=\"Health\")\n",
    "    df7.to_excel(writer,sheet_name=\"Brain_Enhancement_Food\")\n",
    "    df8.to_excel(writer,sheet_name=\"Others\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = [df1[['_id','Sentiment']],\n",
    "           df2[['_id','Sentiment']],\n",
    "           df3[['_id','Sentiment']],\n",
    "           df4[['_id','Sentiment']],\n",
    "           df5[['_id','Sentiment']],\n",
    "           df6[['_id','Sentiment']],\n",
    "           df7[['_id','Sentiment']],\n",
    "           df8[['_id','Sentiment']]]\n",
    "\n",
    "final_comb_df = pd.concat(comb_df)\n",
    "final_comb_df=final_comb_df.reset_index(drop=True)  # Final Sentiment dataframe combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_comb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup code\n",
    "#if \"Sentiment_Collection\" in list(Wholepreneurs_db.list_collection_names()) == True:  #check if collection \"Sentiment_Collection\" exist\n",
    "#    Wholepreneurs_db.drop_collection('Sentiment_Collection')\n",
    "#    results = Sentiment_Collection.insert_many(final_comb_df.T.to_dict().values())\n",
    "#else:\n",
    "#    results = Sentiment_Collection.insert_many(final_comb_df.T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Sentiment_Collection\" in list(Wholepreneurs_db.list_collection_names()):  #check if collection \"Sentiment_Collection\" exist\n",
    "    Wholepreneurs_db.drop_collection('Sentiment_Collection')\n",
    "    results = Sentiment_Collection.insert_many(final_comb_df.T.to_dict().values())\n",
    "else:\n",
    "    results = Sentiment_Collection.insert_many(final_comb_df.T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Positive Sentiment 1\n",
      "Positive Sentiment Percentage = 51%\n",
      "Overall Positive Sentiment 2\n",
      "Positive Sentiment Percentage = 36%\n",
      "Overall Positive Sentiment 3\n",
      "Positive Sentiment Percentage = 84%\n",
      "Overall Positive Sentiment 4\n",
      "Positive Sentiment Percentage = 36%\n",
      "Overall Positive Sentiment 5\n",
      "Positive Sentiment Percentage = 66%\n",
      "Overall Positive Sentiment 6\n",
      "Positive Sentiment Percentage = 54%\n",
      "Overall Positive Sentiment 7\n",
      "Positive Sentiment Percentage = 100%\n",
      "Overall Positive Sentiment 8\n",
      "Positive Sentiment Percentage = 100%\n"
     ]
    }
   ],
   "source": [
    "#==\n",
    "print(\"Overall Positive Sentiment 1\")\n",
    "print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels1.count('pos')//len(NBResultLabels1)) + \"%\")\n",
    "\n",
    "#==\n",
    "print(\"Overall Positive Sentiment 2\")\n",
    "print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels2.count('pos')//len(NBResultLabels2)) + \"%\")\n",
    "\n",
    "#==\n",
    "print(\"Overall Positive Sentiment 3\")\n",
    "print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels3.count('pos')//len(NBResultLabels3)) + \"%\")\n",
    "\n",
    "#==\n",
    "print(\"Overall Positive Sentiment 4\")\n",
    "print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels4.count('pos')//len(NBResultLabels4)) + \"%\")\n",
    "\n",
    "#==\n",
    "print(\"Overall Positive Sentiment 5\")\n",
    "print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels5.count('pos')//len(NBResultLabels5)) + \"%\")\n",
    "\n",
    "#==\n",
    "print(\"Overall Positive Sentiment 6\")\n",
    "print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels6.count('pos')//len(NBResultLabels6)) + \"%\")\n",
    "\n",
    "#==\n",
    "print(\"Overall Positive Sentiment 7\")\n",
    "print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels7.count('pos')//len(NBResultLabels7)) + \"%\")\n",
    "\n",
    "#==\n",
    "print(\"Overall Positive Sentiment 8\")\n",
    "if len(NBResultLabels8) != 0:\n",
    "    print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels8.count('pos')//len(NBResultLabels8)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Creating JSON export files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    with open('Vegan_Organic_Collection.json', 'w') as file1:\n",
    "        file1.write('[')\n",
    "        for document in Vegan_Organic_Collection.find({}):\n",
    "            file1.write(dumps(document))\n",
    "            file1.write(',')\n",
    "        file1.write(']') \n",
    "    with open('Competitors_Collection.json', 'w') as file1:\n",
    "        file1.write('[')\n",
    "        for document in Competitors_Collection.find({}):\n",
    "            file1.write(dumps(document))\n",
    "            file1.write(',')\n",
    "        file1.write(']')\n",
    "    with open('Workout_Collection.json', 'w') as file:\n",
    "        file.write('[')\n",
    "        for document in Workout_Collection.find({}):\n",
    "            file.write(dumps(document))\n",
    "            file.write(',')\n",
    "        file.write(']')   \n",
    "    with open('Meal_Replacement_Collection.json', 'w') as file:\n",
    "        file.write('[')\n",
    "        for document in Meal_Replacement_Collection.find({}):\n",
    "            file.write(dumps(document))\n",
    "            file.write(',')\n",
    "        file.write(']')     \n",
    "    with open('Stress_Relief_Collection.json', 'w') as file:\n",
    "        file.write('[')\n",
    "        for document in Stress_Relief_Collection.find({}):\n",
    "            file.write(dumps(document))\n",
    "            file.write(',')\n",
    "        file.write(']')   \n",
    "    with open('Health_Collection.json', 'w') as file:\n",
    "        file.write('[')\n",
    "        for document in Health_Collection.find({}):\n",
    "            file.write(dumps(document))\n",
    "            file.write(',')\n",
    "        file.write(']')    \n",
    "    with open('Brain_Enhancement_Food_Collection.json', 'w') as file:\n",
    "        file.write('[')\n",
    "        for document in Brain_Enhancement_Food_Collection.find({}):\n",
    "            file.write(dumps(document))\n",
    "            file.write(',')\n",
    "        file.write(']')  \n",
    "    with open('Others_Collection.json', 'w') as file:\n",
    "        file.write('[')\n",
    "        for document in Others_Collection.find({}):\n",
    "            file.write(dumps(document))\n",
    "            file.write(',')\n",
    "        file.write(']')\n",
    "    with open('Sentiment_Collection.json', 'w') as file:\n",
    "        file.write('[')\n",
    "        for document in Sentiment_Collection.find({}):\n",
    "            file.write(dumps(document))\n",
    "            file.write(',')\n",
    "        file.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
